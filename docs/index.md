<span class="fs-4">
[Paper](https://arxiv.org/abs/2204.05428){: .btn .btn-blue .mr-2}
[Code](https://github.com/KeremZaman/explaiNLI){: .btn .btn-purple .mr-2}
[Data](https://github.com/KeremZaman/explaiNLI){: .btn .btn-green .mr-2}
</span>

## Abstract
Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of plausibility and faithfulness properties. First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the potential downsides of erasure-based evaluations. We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods. Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, which may support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.